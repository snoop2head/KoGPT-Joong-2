{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/kor-3-line-poetry\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# root path\n",
    "ROOT_PATH = os.path.abspath(\".\") # this makes compatible absolute path both for local and server\n",
    "\n",
    "# designate root path for the data\n",
    "DATA_ROOT_PATH = os.path.join(ROOT_PATH, 'data')\n",
    "\n",
    "# designate path for each dataset files\n",
    "LYRIC_PATH = os.path.join(DATA_ROOT_PATH, \"lyrics_kor.txt\")\n",
    "BILLBOARD_PATH = os.path.join(DATA_ROOT_PATH, \"rawdata_김지훈_201500844.tsv\")\n",
    "GEULSTAGRAM_PATH = os.path.join(DATA_ROOT_PATH, \"geulstagram.csv\")\n",
    "\n",
    "print(ROOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DEBUG': False, 'num_workers': 4, 'train_batch_size': 16, 'user_name': 'snoop2head', 'file_base_name': 'snoop2head_1117_11:22', 'model_dir': 'skt/ko-gpt-trinity-1.2B-v0.5', 'max_token_length': 42, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'save_steps': 500, 'early_stopping_patience': 5, 'warmup_steps': 500, 'logging_steps': 100, 'evaluation_strategy': 'epoch', 'evaluation_steps': 500, 'result_dir': '/opt/ml/kor-3-line-poetry/results', 'saved_model_dir': '/opt/ml/kor-3-line-poetry/best_models', 'logging_dir': '/opt/ml/kor-3-line-poetry/logs', 'baseline_dir': '/opt/ml/kor-3-line-poetry/baseline-code'}\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from easydict import EasyDict\n",
    "\n",
    "# Initialize configuration\n",
    "CFG = EasyDict()\n",
    "\n",
    "# Dataset Config as constants\n",
    "CFG.DEBUG = False\n",
    "CFG.num_workers = 4\n",
    "CFG.train_batch_size = 16\n",
    "\n",
    "# Train configuration\n",
    "CFG.user_name = \"snoop2head\"\n",
    "today = datetime.now().strftime(\"%m%d_%H:%M\")\n",
    "CFG.file_base_name = f\"{CFG.user_name}_{today}\"\n",
    "CFG.model_dir = \"skt/ko-gpt-trinity-1.2B-v0.5\" # designate the model's name registered on huggingface: https://huggingface.co/skt/ko-gpt-trinity-1.2B-v0.5\n",
    "CFG.max_token_length = 42\n",
    "CFG.learning_rate = 5e-5\n",
    "CFG.weight_decay = 1e-2 # https://paperswithcode.com/method/weight-decay\n",
    "\n",
    "# training steps configurations\n",
    "CFG.save_steps = 500\n",
    "CFG.early_stopping_patience = 5\n",
    "CFG.warmup_steps = 500\n",
    "CFG.logging_steps = 100\n",
    "CFG.evaluation_strategy = 'epoch'\n",
    "CFG.evaluation_steps = 500\n",
    "\n",
    "# Directory configuration\n",
    "CFG.result_dir = os.path.join(ROOT_PATH, \"results\")\n",
    "CFG.saved_model_dir = os.path.join(ROOT_PATH, \"best_models\")\n",
    "CFG.logging_dir = os.path.join(ROOT_PATH, \"logs\")\n",
    "CFG.baseline_dir = os.path.join(ROOT_PATH, 'baseline-code')\n",
    "\n",
    "print(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "def seed_everything(seed) :\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read txt file from line by line\n",
    "def read_txt(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    return lines\n",
    "\n",
    "# make sampling function from the list\n",
    "def sampling(list_lines:list, n:int) -> list:\n",
    "    # sampling\n",
    "    list_lines = np.random.choice(list_lines, n)\n",
    "    list_lines = list(list_lines)\n",
    "    return list_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:694: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, AutoModelWithLMHead\n",
    "\n",
    "# CFG.saved_model_dir = \"./results\"\n",
    "CFG.model_dir = \"snoop2head/KoGPT-Joong-2\"\n",
    "\n",
    "# Attach Language model Head to the pretrained GPT model\n",
    "model = AutoModelWithLMHead.from_pretrained(CFG.model_dir) # KoGPT3 shares the same structure as KoGPT2. \n",
    "\n",
    "\n",
    "# move the model to device\n",
    "if torch.cuda.is_available() and CFG.DEBUG == False:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif CFG.DEBUG == True:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 17 11:23:54 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   38C    P0    37W / 250W |   5823MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "if device == torch.device(\"cuda:0\"):\n",
    "    os.system(\"nvidia-smi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349fa5d997594548a4e2171cb8a3f0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/515 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597672fd41bf47f7ad73a7f857684801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/109 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b17fff0285d4a6bbe7f684a432db4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.00M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, PreTrainedTokenizerFast, AutoTokenizer\n",
    "\n",
    "# https://huggingface.co/transformers/preprocessing.html\n",
    "# Load the Tokenizer: \"Fast\" means that the tokenizer code is written in Rust Lang\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CFG.model_dir,\n",
    "    max_len = CFG.max_token_length,\n",
    "    padding='max_length',\n",
    "    add_special_tokens = True,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation = True,\n",
    "    bos_token = \"<s>\",\n",
    "    eos_token = \"</s>\",\n",
    "    unk_token = \"<unk>\",\n",
    "    pad_token = \"<pad>\",\n",
    "    mask_token = \"<mask>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred sentences given '너는 나의'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['너는 나의 오늘 위로',\n",
       " '너는 나의 속을 볼 수 있는 사람이었으면 한다',\n",
       " '너는 나의 옷자락이고 머릿결이고 꿈결이고 나를 헤집던 사정없는 풍속이었다.',\n",
       " '너는 나의 태양 네가 나눈 빛이면 나는 달빛에도 그을음이 피었다.',\n",
       " '너는 나의 거짓말. 나는 너의 참말. 너를 잊었다는 나와 나를 잊었다는 너의 차이.',\n",
       " '너는 나의 다운타운 베이비야',\n",
       " '너는 나의 달이야',\n",
       " '너는 나의 사랑이고 나의 자랑이다',\n",
       " '너는 나의 맘을 언제나 지배하잖아',\n",
       " '너는 나의 태양 네가 나눈 빛이면 나는 달빛에도 그을려. 달빛 밤보다 까맣게']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def infer_sentence(input_sentence, k, output_token_length):\n",
    "\n",
    "    # encode the sample sentence\n",
    "    input_ids = tokenizer.encode(\n",
    "        input_sentence, \n",
    "        add_special_tokens=False, \n",
    "        return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    # decode the output sequence and print its outcome\n",
    "    list_decoded_sequences = []\n",
    "    while len(list_decoded_sequences) < k:\n",
    "        # generate output sequence from the given encoded input sequence\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=input_ids.to(device), \n",
    "            do_sample=True, \n",
    "            max_length=output_token_length, \n",
    "            num_return_sequences=k\n",
    "            )\n",
    "\n",
    "        for index, generated_sequence in enumerate(output_sequences):\n",
    "            generated_sequence = generated_sequence.tolist()\n",
    "            # remove padding from the generated sequence\n",
    "            generated_sequence = generated_sequence[:generated_sequence.index(tokenizer.pad_token_id)]\n",
    "            decoded_sequence = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "            # print(f\"{index} : {decoded_sequence}\")\n",
    "            list_decoded_sequences.append(decoded_sequence)\n",
    "        list_decoded_sequences = list(set(list_decoded_sequences))\n",
    "    \n",
    "    return list_decoded_sequences\n",
    "\n",
    "input_sentence = \"너는 나의\"\n",
    "print(f\"Inferred sentences given '{input_sentence}'\")\n",
    "inferred_sentences = infer_sentence(input_sentence, k=10, output_token_length=CFG.max_token_length)\n",
    "inferred_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['자그맣고 메마른 씨앗', '탄 것 말고 좋은 것 더 드셨음 지금 곁에 계시나.', '풍경에 덧입혀진 지친 내 모습']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_samhaengshi(input_letter, k, output_token_length):\n",
    "    list_samhaengshi = []\n",
    "    for one_letter in input_letter:\n",
    "        list_decoded_sequences = infer_sentence(one_letter, k=k, output_token_length=output_token_length)\n",
    "        list_samhaengshi.extend(list_decoded_sequences)\n",
    "    return list_samhaengshi\n",
    "\n",
    "make_samhaengshi(input_letter=\"자탄풍\", k=1, output_token_length=CFG.max_token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_residual_samhaengshi(input_letter, k, output_token_length):\n",
    "    # make letter string into \n",
    "    list_samhaengshi = []\n",
    "    \n",
    "    # initializing text and index for iteration purpose\n",
    "    index = 0\n",
    "\n",
    "    # iterating over the input letter string\n",
    "    for index, letter_item in enumerate(input_letter):\n",
    "        # initializing the input_letter\n",
    "        if index == 0:\n",
    "            residual_text = letter_item\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # infer and add to the output\n",
    "        list_sentences = infer_sentence(residual_text, k, output_token_length)\n",
    "        inferred_sentence = list_sentences[0] # first item of the inferred list\n",
    "        if index != 0:\n",
    "            # remove previous sentence from the output\n",
    "            inferred_sentence = inferred_sentence.replace(list_samhaengshi[index-1], \"\").strip() \n",
    "        else:\n",
    "            pass\n",
    "        list_samhaengshi.append(inferred_sentence)\n",
    "        \n",
    "        # until the end of the input_letter, give the previous residual_text to the next iteration\n",
    "        if index < len(input_letter) - 1: \n",
    "            residual_sentence = list_samhaengshi[index]\n",
    "            next_letter = input_letter[index + 1]\n",
    "            residual_text = f\"{residual_sentence} {next_letter}\" #  previous sentence + next letter\n",
    "            # print(residual_text)\n",
    "\n",
    "        elif index == len(input_letter) - 1: # end of the input_letter\n",
    "            # Concatenate strings in the list without intersection\n",
    "\n",
    "            return list_samhaengshi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_item = \"연세대\" \n",
    "inferred_samhaengshi = make_residual_samhaengshi(sample_item, k=1, output_token_length=CFG.max_token_length)\n",
    "for item in inferred_samhaengshi:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo\n",
    "- probably make candidate 10 sentences per letter and pick sentences with sentence transformer trained with Next Sentence Prediction Task?\n",
    "- Filter out similar sentences based on levenstein distance or sentence bert\n",
    "- remove curse words, person words with pororo or other tools -> either from dataset or inference process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/lovit/levenshtein_finder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
