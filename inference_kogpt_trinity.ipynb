{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/noopy/Documents/BERT-PROJECTS/kor-3-line-poetry\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# root path\n",
    "ROOT_PATH = os.path.abspath(\".\") # this makes compatible absolute path both for local and server\n",
    "\n",
    "# designate root path for the data\n",
    "DATA_ROOT_PATH = os.path.join(ROOT_PATH, 'data')\n",
    "\n",
    "# designate path for each dataset files\n",
    "LYRIC_PATH = os.path.join(DATA_ROOT_PATH, \"lyrics_kor.txt\")\n",
    "BILLBOARD_PATH = os.path.join(DATA_ROOT_PATH, \"rawdata_김지훈_201500844.tsv\")\n",
    "GEULSTAGRAM_PATH = os.path.join(DATA_ROOT_PATH, \"geulstagram.csv\")\n",
    "\n",
    "print(ROOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DEBUG': True, 'num_workers': 4, 'train_batch_size': 16, 'user_name': 'snoop2head', 'file_base_name': 'snoop2head_1116_14:24', 'model_dir': 'skt/ko-gpt-trinity-1.2B-v0.5', 'max_token_length': 42, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'save_steps': 500, 'early_stopping_patience': 5, 'warmup_steps': 500, 'logging_steps': 100, 'evaluation_strategy': 'epoch', 'evaluation_steps': 500, 'result_dir': '/Users/noopy/Documents/BERT-PROJECTS/kor-3-line-poetry/results', 'saved_model_dir': '/Users/noopy/Documents/BERT-PROJECTS/kor-3-line-poetry/best_models', 'logging_dir': '/Users/noopy/Documents/BERT-PROJECTS/kor-3-line-poetry/logs', 'baseline_dir': '/Users/noopy/Documents/BERT-PROJECTS/kor-3-line-poetry/baseline-code'}\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from easydict import EasyDict\n",
    "\n",
    "# Initialize configuration\n",
    "CFG = EasyDict()\n",
    "\n",
    "# Dataset Config as constants\n",
    "CFG.DEBUG = True\n",
    "CFG.num_workers = 4\n",
    "CFG.train_batch_size = 16\n",
    "\n",
    "# Train configuration\n",
    "CFG.user_name = \"snoop2head\"\n",
    "today = datetime.now().strftime(\"%m%d_%H:%M\")\n",
    "CFG.file_base_name = f\"{CFG.user_name}_{today}\"\n",
    "CFG.model_dir = \"skt/ko-gpt-trinity-1.2B-v0.5\" # designate the model's name registered on huggingface: https://huggingface.co/skt/ko-gpt-trinity-1.2B-v0.5\n",
    "CFG.max_token_length = 42\n",
    "CFG.learning_rate = 5e-5\n",
    "CFG.weight_decay = 1e-2 # https://paperswithcode.com/method/weight-decay\n",
    "\n",
    "# training steps configurations\n",
    "CFG.save_steps = 500\n",
    "CFG.early_stopping_patience = 5\n",
    "CFG.warmup_steps = 500\n",
    "CFG.logging_steps = 100\n",
    "CFG.evaluation_strategy = 'epoch'\n",
    "CFG.evaluation_steps = 500\n",
    "\n",
    "# Directory configuration\n",
    "CFG.result_dir = os.path.join(ROOT_PATH, \"results\")\n",
    "CFG.saved_model_dir = os.path.join(ROOT_PATH, \"best_models\")\n",
    "CFG.logging_dir = os.path.join(ROOT_PATH, \"logs\")\n",
    "CFG.baseline_dir = os.path.join(ROOT_PATH, 'baseline-code')\n",
    "\n",
    "print(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "def seed_everything(seed) :\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read txt file from line by line\n",
    "def read_txt(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    return lines\n",
    "\n",
    "# make sampling function from the list\n",
    "def sampling(list_lines:list, n:int) -> list:\n",
    "    # sampling\n",
    "    list_lines = np.random.choice(list_lines, n)\n",
    "    list_lines = list(list_lines)\n",
    "    return list_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "CFG.saved_model_dir = \"./results\"\n",
    "# model_path = \"/Users/noopy/Documents/BERT-PROJECTS/kor-3-line-poetry/results/snoop2head_1114_05_58_loss_0.3655.pt\"\n",
    "CFG.model_dir = \"/Users/noopy/Documents/BERT-PROJECTS/kor-3-line-poetry/results/checkpoint-18200\"\n",
    "\n",
    "# Attach Language model Head to the pretrained GPT model\n",
    "model = GPT2LMHeadModel.from_pretrained(CFG.model_dir) # KoGPT3 shares the same structure as KoGPT2. \n",
    "\n",
    "# move the model to device\n",
    "if torch.cuda.is_available() and CFG.DEBUG == False:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    # model.load_state_dict(torch.load(model_path))\n",
    "elif CFG.DEBUG == True:\n",
    "    device = torch.device(\"cpu\")\n",
    "    # model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == torch.device(\"cuda:0\"):\n",
    "    os.system(\"nvidia-smi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "CFG.model_dir = \"skt/ko-gpt-trinity-1.2B-v0.5\" # designate the model's name registered on huggingface: https://huggingface.co/skt/ko-gpt-trinity-1.2B-v0.5\n",
    "\n",
    "# https://huggingface.co/transformers/preprocessing.html\n",
    "# Load the Tokenizer: \"Fast\" means that the tokenizer code is written in Rust Lang\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    CFG.model_dir,\n",
    "    max_len = CFG.max_token_length,\n",
    "    padding='max_length',\n",
    "    add_special_tokens = True,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation = True,\n",
    "    bos_token = \"<s>\",\n",
    "    eos_token = \"</s>\",\n",
    "    unk_token = \"<unk>\",\n",
    "    pad_token = \"<pad>\",\n",
    "    mask_token = \"<mask>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred sentences given '나는 너의'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['나는 너의 모든 시간을 알고 싶다',\n",
       " '나는 너의 결핍이면 곳곳 너를 두곤 했다.',\n",
       " '나는 너의 척추를 꼭꼭 싶어먹었다.',\n",
       " '나는 너의 이름을 그 누가 보아도 아름답고 소중한 사람이라 부를 수 있겠다',\n",
       " '나는 너의 모든 것을 담고 싶을 만큼 너를 좋아했었다.',\n",
       " '나는 너의 마음을 읽지 못했고 늘 같은 말만 했다',\n",
       " '나는 너의 그 손을 잡고 싶었지만 차마 두 손으로 너의 머리를 감싸고 말았다',\n",
       " '나는 너의 시가 되고 싶다.',\n",
       " '나는 너의 마음을 정하지마',\n",
       " '나는 너의 그 미소는',\n",
       " '나는 너의 머리를 살짝 쥐어박고 말했다']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def infer_sentence(input_sentence, k, output_token_length):\n",
    "\n",
    "    # encode the sample sentence\n",
    "    input_ids = tokenizer.encode(\n",
    "        input_sentence, \n",
    "        add_special_tokens=False, # CLS, SEP, PAD, UNK\n",
    "        return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    # decode the output sequence and print its outcome\n",
    "    list_decoded_sequences = []\n",
    "    while len(list_decoded_sequences) < k:\n",
    "        # generate output sequence from the given encoded input sequence\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=input_ids.to(device), \n",
    "            do_sample=True, \n",
    "            max_length=output_token_length, \n",
    "            num_return_sequences=k\n",
    "            )\n",
    "\n",
    "        for index, generated_sequence in enumerate(output_sequences):\n",
    "            generated_sequence = generated_sequence.tolist()\n",
    "            # remove padding from the generated sequence\n",
    "            generated_sequence = generated_sequence[:generated_sequence.index(tokenizer.pad_token_id)]\n",
    "            decoded_sequence = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "            # print(f\"{index} : {decoded_sequence}\")\n",
    "            list_decoded_sequences.append(decoded_sequence)\n",
    "        list_decoded_sequences = list(set(list_decoded_sequences))\n",
    "    \n",
    "    return list_decoded_sequences\n",
    "\n",
    "input_sentence = \"나는 너의\"\n",
    "print(f\"Inferred sentences given '{input_sentence}'\")\n",
    "inferred_sentences = infer_sentence(input_sentence, k=10, output_token_length=CFG.max_token_length)\n",
    "inferred_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['자정을 알리는 첫 차 소리', '탄 것 말고 좋은 것 더 드셨음 지금 곁에 계시나.', '풍차가 도는 조각 공원 곱게 수놓인']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_samhaengshi(input_letter, k, output_token_length):\n",
    "    list_samhaengshi = []\n",
    "    for one_letter in input_letter:\n",
    "        list_decoded_sequences = infer_sentence(one_letter, k=k, output_token_length=output_token_length)\n",
    "        list_samhaengshi.extend(list_decoded_sequences)\n",
    "    return list_samhaengshi\n",
    "\n",
    "make_samhaengshi(input_letter=\"자탄풍\", k=1, output_token_length=CFG.max_token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_residual_samhaengshi(input_letter, k, output_token_length):\n",
    "    # make letter string into \n",
    "    list_samhaengshi = []\n",
    "    \n",
    "    # initializing text and index for iteration purpose\n",
    "    index = 0\n",
    "\n",
    "    # iterating over the input letter string\n",
    "    for index, letter_item in enumerate(input_letter):\n",
    "        # initializing the input_letter\n",
    "        if index == 0:\n",
    "            residual_text = letter_item\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # infer and add to the output\n",
    "        list_sentences = infer_sentence(residual_text, k, output_token_length) #  옥 -> [옥석은 세월이 가려내리라.], \"옥석은 세월이 가려내리라. 수\" -> []\n",
    "        inferred_sentence = list_sentences[0] # first item of the inferred list\n",
    "        if index != 0:\n",
    "            # remove previous sentence from the output\n",
    "            inferred_sentence = inferred_sentence.replace(list_samhaengshi[index-1], \"\").strip() \n",
    "        else:\n",
    "            pass\n",
    "        list_samhaengshi.append(inferred_sentence)\n",
    "        \n",
    "        # until the end of the input_letter, give the previous residual_text to the next iteration\n",
    "        if index < len(input_letter) - 1: \n",
    "            residual_sentence = list_samhaengshi[index]\n",
    "            next_letter = input_letter[index + 1]\n",
    "            residual_text = f\"{residual_sentence} {next_letter}\" # previous sentence + next letter # \"옥석은 세월이 가려내리라. 수\"\n",
    "            # print(residual_text)\n",
    "\n",
    "        elif index == len(input_letter) - 1: # end of the input_letter\n",
    "            # Concatenate strings in the list without intersection\n",
    "\n",
    "            return list_samhaengshi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "옥석은 세월이 가려내리라.\n",
      "수줍은 아이처럼\n",
      "수줍은 얼굴 하고 흰 눈만\n",
      "수줍게 쌓였네.\n",
      "염라대왕은 청년을 노려보며 착하게 살긴 살았는데 시간을 덧없이 보냈구나.\n",
      "차돌\n"
     ]
    }
   ],
   "source": [
    "sample_item = \"옥수수수염차\" \n",
    "inferred_samhaengshi = make_residual_samhaengshi(sample_item, k=1, output_token_length=CFG.max_token_length)\n",
    "for item in inferred_samhaengshi:\n",
    "    print(item)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo\n",
    "- probably make candidate 10 sentences per letter and pick sentences with sentence transformer trained with Next Sentence Prediction Task?\n",
    "- Filter out similar sentences based on levenstein distance or sentence bert\n",
    "- remove curse words, person words with pororo or other tools -> either from dataset or inference process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/lovit/levenshtein_finder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
