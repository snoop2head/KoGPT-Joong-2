{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/noopy/Documents/BERT-PROJECTS/kor-3-line-poetry\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# root path\n",
    "ROOT_PATH = os.path.abspath(\".\") # this makes compatible absolute path both for local and server\n",
    "\n",
    "# designate root path for the data\n",
    "DATA_ROOT_PATH = os.path.join(ROOT_PATH, 'data')\n",
    "\n",
    "# designate path for each dataset files\n",
    "LYRIC_PATH = os.path.join(DATA_ROOT_PATH, \"lyrics_kor.txt\")\n",
    "BILLBOARD_PATH = os.path.join(DATA_ROOT_PATH, \"rawdata_김지훈_201500844.tsv\")\n",
    "GEULSTAGRAM_PATH = os.path.join(DATA_ROOT_PATH, \"geulstagram.csv\")\n",
    "\n",
    "print(ROOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DEBUG': True, 'num_workers': 4, 'train_batch_size': 16, 'user_name': 'snoop2head', 'file_base_name': 'snoop2head_1114_23:29', 'model_dir': 'skt/ko-gpt-trinity-1.2B-v0.5', 'max_token_length': 42, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'save_steps': 500, 'early_stopping_patience': 5, 'warmup_steps': 500, 'logging_steps': 100, 'evaluation_strategy': 'epoch', 'evaluation_steps': 500, 'result_dir': '/Users/noopy/Documents/BERT-PROJECTS/kor-3-line-poetry/results', 'saved_model_dir': '/Users/noopy/Documents/BERT-PROJECTS/kor-3-line-poetry/best_models', 'logging_dir': '/Users/noopy/Documents/BERT-PROJECTS/kor-3-line-poetry/logs', 'baseline_dir': '/Users/noopy/Documents/BERT-PROJECTS/kor-3-line-poetry/baseline-code'}\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from easydict import EasyDict\n",
    "\n",
    "# Initialize configuration\n",
    "CFG = EasyDict()\n",
    "\n",
    "# Dataset Config as constants\n",
    "CFG.DEBUG = True\n",
    "CFG.num_workers = 4\n",
    "CFG.train_batch_size = 16\n",
    "\n",
    "# Train configuration\n",
    "CFG.user_name = \"snoop2head\"\n",
    "today = datetime.now().strftime(\"%m%d_%H:%M\")\n",
    "CFG.file_base_name = f\"{CFG.user_name}_{today}\"\n",
    "CFG.model_dir = \"skt/ko-gpt-trinity-1.2B-v0.5\" # designate the model's name registered on huggingface: https://huggingface.co/skt/ko-gpt-trinity-1.2B-v0.5\n",
    "CFG.max_token_length = 42\n",
    "CFG.learning_rate = 5e-5\n",
    "CFG.weight_decay = 1e-2 # https://paperswithcode.com/method/weight-decay\n",
    "\n",
    "# training steps configurations\n",
    "CFG.save_steps = 500\n",
    "CFG.early_stopping_patience = 5\n",
    "CFG.warmup_steps = 500\n",
    "CFG.logging_steps = 100\n",
    "CFG.evaluation_strategy = 'epoch'\n",
    "CFG.evaluation_steps = 500\n",
    "\n",
    "# Directory configuration\n",
    "CFG.result_dir = os.path.join(ROOT_PATH, \"results\")\n",
    "CFG.saved_model_dir = os.path.join(ROOT_PATH, \"best_models\")\n",
    "CFG.logging_dir = os.path.join(ROOT_PATH, \"logs\")\n",
    "CFG.baseline_dir = os.path.join(ROOT_PATH, 'baseline-code')\n",
    "\n",
    "print(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "def seed_everything(seed) :\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read txt file from line by line\n",
    "def read_txt(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    return lines\n",
    "\n",
    "# make sampling function from the list\n",
    "def sampling(list_lines:list, n:int) -> list:\n",
    "    # sampling\n",
    "    list_lines = np.random.choice(list_lines, n)\n",
    "    list_lines = list(list_lines)\n",
    "    return list_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "CFG.saved_model_dir = \"./results\"\n",
    "model_path = \"/Users/noopy/Documents/BERT-PROJECTS/kor-3-line-poetry/results/snoop2head_1114_05_58_loss_0.3655.pt\"\n",
    "\n",
    "# Attach Language model Head to the pretrained GPT model\n",
    "model = GPT2LMHeadModel.from_pretrained(CFG.model_dir) # KoGPT3 shares the same structure as KoGPT2. \n",
    "\n",
    "# move the model to device\n",
    "if torch.cuda.is_available() and CFG.DEBUG == False:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "elif CFG.DEBUG == True:\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == torch.device(\"cuda:0\"):\n",
    "    os.system(\"nvidia-smi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "# https://huggingface.co/transformers/preprocessing.html\n",
    "# Load the Tokenizer: \"Fast\" means that the tokenizer code is written in Rust Lang\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    CFG.model_dir,\n",
    "    max_len = CFG.max_token_length,\n",
    "    padding='max_length',\n",
    "    add_special_tokens = True,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation = True,\n",
    "    bos_token = \"<s>\",\n",
    "    eos_token = \"</s>\",\n",
    "    unk_token = \"<unk>\",\n",
    "    pad_token = \"<pad>\",\n",
    "    mask_token = \"<mask>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred sentences given '그대 왜 내 꿈에'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['그대 왜 내 꿈에 나타나지 않는 걸까',\n",
       " '그대 왜 내 꿈에 나타나지 않았나',\n",
       " '그대 왜 내 꿈에 나타나지 않았을까 이유 모를 눈물이 흐른다',\n",
       " '그대 왜 내 꿈에 나오지 않는 걸까?',\n",
       " '그대 왜 내 꿈에 나오질 않는 거야',\n",
       " '그대 왜 내 꿈에 안 나왔죠',\n",
       " '그대 왜 내 꿈에 오지 않으려 하는 걸까요',\n",
       " '그대 왜 내 꿈에 나타나지 않았을까',\n",
       " '그대 왜 내 꿈에 오지 않는 거죠',\n",
       " '그대 왜 내 꿈에 오지 않죠',\n",
       " '그대 왜 내 꿈에 나타나지 않았지',\n",
       " '그대 왜 내 꿈에 나타나지 않았을까 후회',\n",
       " '그대 왜 내 꿈에 안오죠 지나간 글']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence = \"그대 왜 내 꿈에\"\n",
    "\n",
    "def infer_sentence(input_sentence, k, output_token_length):\n",
    "\n",
    "    # encode the sample sentence\n",
    "    input_ids = tokenizer.encode(\n",
    "        input_sentence, \n",
    "        add_special_tokens=False, \n",
    "        return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    # decode the output sequence and print its outcome\n",
    "    list_decoded_sequences = []\n",
    "    while len(list_decoded_sequences) < k:\n",
    "        # generate output sequence from the given encoded input sequence\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=input_ids.to(device), \n",
    "            do_sample=True, \n",
    "            max_length=output_token_length, \n",
    "            num_return_sequences=k\n",
    "            )\n",
    "\n",
    "        for index, generated_sequence in enumerate(output_sequences):\n",
    "            generated_sequence = generated_sequence.tolist()\n",
    "            # remove padding from the generated sequence\n",
    "            generated_sequence = generated_sequence[:generated_sequence.index(tokenizer.pad_token_id)]\n",
    "            decoded_sequence = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "            # print(f\"{index} : {decoded_sequence}\")\n",
    "            list_decoded_sequences.append(decoded_sequence)\n",
    "        list_decoded_sequences = list(set(list_decoded_sequences))\n",
    "    \n",
    "    return list_decoded_sequences\n",
    "\n",
    "print(f\"Inferred sentences given '{input_sentence}'\")\n",
    "infer_sentence(input_sentence, k=7, output_token_length=CFG.max_token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['자라고 시간은 흐르고 당신은 변하고 난 당신으로부터 도망치고 있네',\n",
       " '탄탄한 인생보다 알찬 인생이 어디 있어',\n",
       " '풍파 없는 항해 얼마나 단조로운가.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_samhaengshi(input_letter, k, output_token_length):\n",
    "    list_samhaengshi = []\n",
    "    for one_letter in input_letter:\n",
    "        list_decoded_sequences = infer_sentence(one_letter, k=k, output_token_length=output_token_length)\n",
    "        list_samhaengshi.extend(list_decoded_sequences)\n",
    "    return list_samhaengshi\n",
    "\n",
    "make_samhaengshi(input_letter=\"자탄풍\", k=1, output_token_length=CFG.max_token_length)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
